记录在ChatGPT生成文本检测器：比赛中，学习到的使用NLP的技术和模型等：
https://challenge.xfyun.cn/topic/info?type=text-detector&ch=ymfk4uU

NLP：chatgpt生成文本检测：本质：文本分类。   文本分类大类：
    Baseline的基本思路：https://datawhaler.feishu.cn/docx/NehLd4seGoI8AkxKaAxcG3kRnzd
数据处理：
    5大步：
    1： 数据预处理：
     A。 收集真实对话和 ChatGPT 生成的对话的数据集。    （在官网数据集中）
     b. 通过标记文本、删除停用词并将所有文本转换为小写来预处理数据。
     C。 将数据分为训练集、验证集和测试集。              （方便之后进行操作）
    2： 特征提取：
     A。 使用词嵌入等技术（例如 Word2Vec、GloVe）从文本数据中提取特征。
     b. 考虑使用情感分析、主题建模或命名实体识别等技术来提取其他特征。  
    （主要转换为向量的方式）
   3：模型选择和训练：
     A。 选择合适的机器学习算法进行文本分类，例如朴素贝叶斯、逻辑回归或支持向量机。
     b. 使用您选择的特征提取器在预处理数据上训练模型。
     C。 使用网格搜索或随机搜索等技术调整模型的超参数。
    4：评估：
     A。 使用测试集来评估模型的性能。
     b. 测量准确度、精确度、召回率和 F1 分数等指标来评估模型的性能。
     5部署：
     A。 一旦您对模型的性能感到满意，您就可以部署它来对新的、未见过的对话进行分类。
     本赛题预测结果文件按照csv格式提交，编码为UTF-8，第一行为表头
     提交前请确保预测结果的格式与sample_submit.csv中的格式一致

    通过AIstudio部署最后跑出结果保存为csv格式：

代码：TFIDF方法：
首先：对于TF-IDF方法： 1：计算词频（TF）  2：计算逆文档频率   3：计算TF-ID，TF-IDF与一个词在文档中的出现次数成正比，与该词在整个语言中的出现次数成反比。
代码（基于AIstudio）：
一：传统导入： 涉及import : glob numpy pandas sklearn  (Spical:关于Sklearn:
viod exp:

  LogisticRegression 是 scikit-learn 的 sklearn.linear_model 模块中的一个类，scikit-learn 是一个流行的 Python 机器学习库,
  它实现了逻辑回归算法，这是一种用于二元分类问题的监督学习算法。在文本分类的背景下，逻辑回归可用于训练预测给定文本片段的二元标签
  例如垃圾邮件与非垃圾邮件）的模型
  一般的使用方法：准备数据：使用词袋或 TF-IDF 等技术将文本数据转换为数字表示形式，从而对文本数据进行预处理。 将您的数据分为训练集和测试集。
     创建 LogisticRegression 对象：从 sklearn.linear_model 导入 LogisticRegression 类并创建它的实例。
     拟合模型：使用 fit() 方法根据训练数据训练模型。
     预测测试数据：使用predict()方法预测测试数据的标签。
     评估模型：使用准确度、精确度、召回率和 F1 分数等指标来评估模型的性能。

二：准备数据集！
train_data = pd.read_csv('./ChatGPT生成文本检测器公开数据-更新/train.csv')
test_data = pd.read_csv('./ChatGPT生成文本检测器公开数据-更新/test.csv')

viod v1:
  没什么好说的，就是导入训练和测试集

train_data['content'] = train_data['content'].apply(lambda x: x[1:-1])
test_data['content'] = test_data['content'].apply(lambda x: x[1:-1])

viod v1:
  train_data['content'] 是一个 pandas 系列，包含 train_data 数据帧的“content”列中的文本数据。
     apply() 是 pandas 的一种方法，它将给定的函数应用于系列的所有元素。
     本例中使用的函数是 lambda x: x[1:-1]。 此函数将字符串 x 作为输入，并返回通过删除 x 的第一个和最后一个字符而创建的新字符串。
     [] 语法用于对字符串进行切片。 x[1:-1] 表示“从第二个字符（索引 1）开始到倒数第二个字符（索引 -1）”。
因此，train_data['content'].apply(lambda x: x[1:-1]) 的结果是一个包含修改后的文本数据的新系列，其中每个字符串的第一个和最后一个字符已被删除。

三： 特征提取！

# 第1种tfidf参数
tfidf = TfidfVectorizer(token_pattern=r'\w{1}', max_features=2000)
train_tfidf = tfidf.fit_transform(train_data['content'])
test_tfidf = tfidf.fit_transform(test_data['content'])
print(classification_report(
    cross_val_predict(
        LogisticRegression(),
        train_tfidf,
        train_data['label']
    ),
    train_data['label'],
    digits=4
))

# 第2种tfidf参数
tfidf = TfidfVectorizer(token_pattern=r'\w{1}', max_features=5000)
train_tfidf = tfidf.fit_transform(train_data['content'])
test_tfidf = tfidf.fit_transform(test_data['content'])
print(classification_report(
    cross_val_predict(
        LogisticRegression(),
        train_tfidf,
        train_data['label']
    ),
    train_data['label'],
    digits=4
))

# 第3种tfidf参数
tfidf = TfidfVectorizer(token_pattern=r'\w{1}', max_features=5000, ngram_range=(1,2))
train_tfidf = tfidf.fit_transform(train_data['content'])
test_tfidf = tfidf.fit_transform(test_data['content'])
print(classification_report(
    cross_val_predict(
        LogisticRegression(),
        train_tfidf,
        train_data['label']
    ),
    train_data['label'],
    digits=4
))

viod v1:
  代码#1part解释：
     tfidf = TfidfVectorizer(token_pattern=r'\w{1}', max_features=2000)：这将创建一个具有以下参数的 TF-IDF 矢量化器对象：
     token_pattern：此参数指定用于标记文本数据的模式。 在本例中，它设置为 r'\w{1}'，这意味着矢量化器会将文本数据中的每个单词视为单个标记。
     max_features：此参数指定要包含在矢量化中的特征（即标记）的最大数量。 在本例中，它设置为 2000，这意味着矢量化器将考虑文本数据中最多 2000 个唯一标记。
     train_tfidf = tfidf.fit_transform(train_data['content'])：这会将 TF-IDF 矢量化器应用于 train_data['content'] 数组，
     其中包含训练集的文本数据。 生成的数组 train_tfidf 将具有与 train_data['content'] 相同的形状，但具有表示每个标记的 TF-IDF 分数的数值。
     test_tfidf = tfidf.fit_transform(test_data['content'])：这会将 TF-IDF 矢量化器应用于 test_data['content'] 数组，其中包含测试集的文本数据。
     生成的数组 test_tfidf 将具有与 test_data['content'] 相同的形状，但具有表示每个标记的 TF-IDF 分数的数值。
     print(classification_report(cross_val_predict(LogisticRegression(), train_tfidf, train_data['label']), train_data['label'],digits=4))：
     打印在 TF-IDF 上训练的逻辑回归模型的分类报告 转换训练数据。 该报告包括准确性、精确度、召回率和 F1 分数等指标。
     cross_val_predict 函数用于使用 TF-IDF 转换后的数据来预测训练数据的标签，classification_report 函数用于生成报告。 digits=4 参数指定要在报告中显示的小数位数。
     目是使用 TF-IDF 向量化和逻辑回归执行文本分类。 TF-IDF 矢量化器将文本数据转换为可用作逻辑回归模型输入的数值表示，并在 TF-IDF 转换数据上训练模型以预测训练数据的标签。
     然后生成分类报告以评估模型的性能。

四：模型训练、评估与优化

m = LogisticRegression()
    m.fit(
    train_tfidf,
    train_data['label']
)

void v1:
     m = LogisticRegression()：创建 LogisticRegression 类的实例。
     m.fit(train_tfidf, train_data['label'])：在 TF-IDF 转换后的训练数据和相应标签上训练逻辑回归模型。
     fit() 方法将返回经过训练的模型，然后可以使用该模型使用predict() 方法对新数据进行预测。


五：结果输出！
test_data['label'] = m.predict(test_tfidf)
test_data[['name', 'label']].to_csv('tfidf.csv', index=None)

void v1:
  很简单的输出,test_data['label'] = m.predict(test_tfidf) 将测试数据的预测标签分配给 test_data 数据框中名为 label 的新列。
  m.predict(test_tfidf) 部分使用经过训练的逻辑回归模型 m 来预测测试数据的标签，该标签存储在 test_tfidf 变量中。 然后将预测的标签分配给 test_data 数据帧的标签列。
  代码 test_data[['name', 'label']].to_csv('tfidf.csv', index=None) 将带有新标签列的 test_data 数据帧写入名为 tfidf.csv 的 CSV 文件。
  index=None 参数指定数据帧不应在输出文件中包含索引（即行号）。



深度学习版本：
代码解释以及模型理解（Baseline基础版本）
!unzip -o /home/aistudio/data/data232024/ChatGPT生成文本检测器公开数据-更新.zip -d /home/aistudio/data # 解压原始数据集
!cp /home/aistudio/data/data233414/* /home/aistudio/work/ # 复制模型参数文件
import numpy as np # 数值计算
import pandas as pd # 数据分析
from tqdm import tqdm # 进度条显示
import paddle # PaddlePaddle 深度学习框架
from paddlenlp.transformers import AutoModelForSequenceClassification, AutoTokenizer # 飞桨自然语言处理工具包（模型、分词器）
from paddle.io import DataLoader # 数据加载器
from paddlenlp.datasets import MapDataset # 数据集转换
from sklearn.model_selection import train_test_split # 训练集与验证集拆分
import matplotlib.pyplot as plt # 绘图
data = pd.read_csv("/home/aistudio/data/ChatGPT生成文本检测器公开数据-更新/train.csv") # 加载赛事提供的训练数据
test_data = pd.read_csv("/home/aistudio/data/ChatGPT生成文本检测器公开数据-更新/test.csv") # 加载赛事所需提交的测试数据
data.sample(frac=1).head() # 随机查看 5 行训练数据中的内容
# 句子长度分布图
data['content'].apply(len).plot(kind='box')
plt.title('Data')
# 按照 10% 的比例划分训练集与验证集
train_data, valid_data = train_test_split(data, test_size=0.1)

# 下面就是一堆操作，把数据变成数据加载器可以识别的格式，自定义数据集类也是同样的效果
train_dict = train_data.to_dict(orient='records')
valid_dict = valid_data.to_dict(orient='records')
train_ds = MapDataset(train_dict)
valid_ds = MapDataset(valid_dict)

# 将整体数据拆分为 30 份，放入数据加载器，就是一次性会有 <总样本数 / 30> 份数据同时并行计算，份数越多，并行越多，显存占用越大，需要根据需求来选择
train_loader = DataLoader(train_dict, batch_size=30, shuffle=True) # 训练数据可以随机打乱，让模型更好地学习，减轻学习到无关特征的问题
valid_loader = DataLoader(valid_dict, batch_size=30) # 这里用的是 V100 32G，如果是显存更小的卡，需要调小一点，不然会炸显存

# 载入模型与分词器

# 使用 ernie-3.0-mini-zh 序列分类模型，并将分类类别数设置为 2
model = AutoModelForSequenceClassification.from_pretrained("ernie-3.0-mini-zh", num_classes=2)
# 使用 ernie-3.0-mini-zh 分词器
tokenizer = AutoTokenizer.from_pretrained("ernie-3.0-mini-zh")

# 定义 AdamW 优化器，学习率为 0.000001
optimizer = paddle.optimizer.AdamW(1e-5, parameters=model.parameters())

# 定义损失函数为交叉熵函数，计算每个 mini batch 的均值
loss_fn = paddle.nn.loss.CrossEntropyLoss(reduction='mean')

for epoch in range(30): # 训练 30 轮
    # 训练过程
    model.train() # 切换模型为训练模式
    for batch_x in tqdm(train_loader): # 每次从数据加载器读入一批(batch)数据
        X = tokenizer(batch_x["content"], max_length=1015, padding=True) # 将数据转换为模块可处理的数据形式
        input_ids = paddle.to_tensor(X['input_ids'], dtype="int32") # 将 input_ids 变为张量，方便并行计算
        token_type_ids = paddle.to_tensor(X['token_type_ids'], dtype="int32") # 将 token_type_ids 变为张量
        pred = model(input_ids, token_type_ids) # 将数据读入模型，并得到计算后的结果
        loss = loss_fn(pred, paddle.to_tensor(batch_x["label"], dtype="int32")) # 对比预测结果与真实结果，计算损失函数的值
        loss.backward() # 反向传播，计算梯度
        optimizer.step() # 优化器根据梯度与学习率调整模型参数
        optimizer.clear_gradients() # 清空梯度，避免下次梯度计算时累加

    # 验证过程
    model.eval() # 切换模型为验证模式
    val_loss = [] # 验证集数据的损失函数合集
    with paddle.no_grad(): # 在模型验证时，只做前向计算，因此不需要保存梯度信息
        for batch_x in tqdm(valid_loader): # 下面的操作与训练过程相同
            X = tokenizer(batch_x["content"], max_length=1015, padding=True)
            input_ids = paddle.to_tensor(X['input_ids'], dtype="int32")
            token_type_ids = paddle.to_tensor(X['token_type_ids'], dtype="int32")
            pred = model(input_ids, token_type_ids)
            loss = loss_fn(pred, paddle.to_tensor(batch_x["label"], dtype="int32"))
            val_loss.append(loss.item()) # 将计算出的损失函数值存入合集
            
    # 打印本轮训练的验证集损失函数值，与预测正确率
    print('Epoch {0}, Val loss {1:3f}, Val Accuracy {2:3f}'.format(
    epoch,
    np.mean(val_loss), 
    (pred.argmax(1) == batch_x["label"]).astype('float').mean().item()
))
# 保存模型参数
paddle.save(model.state_dict(), "/home/aistudio/work/model.pdparams")
# 保存优化器参数
paddle.save(optimizer.state_dict(), "/home/aistudio/work/optimizer.pdopt")

# 如果你拿到了模型参数（在 AIStudio 中提供），你可以运行这行代码，如果直接运行模型，则没有必要运行

# 载入模型参数、优化器参数的最后一个epoch保存的检查点
layer_state_dict = paddle.load("/home/aistudio/work/model.pdparams")
opt_state_dict = paddle.load("/home/aistudio/work/optimizer.pdopt")

# 将加载后的参数与模型关联起来
model.set_state_dict(layer_state_dict)
optimizer.set_state_dict(opt_state_dict)

# 自定义推理函数
def infer(string: str) -> int:
    """将文本传入模型并返回预测结果
    
    输入：
        - string: str
            待预测的文本内容
    
    输出:
        - result: int
            模型的预测结果
    """
    X = tokenizer([string], max_length=1015, padding=True)
    input_ids = paddle.to_tensor(X['input_ids'], dtype="int32")
    token_type_ids = paddle.to_tensor(X['token_type_ids'], dtype="int32")
    pred = model(input_ids, token_type_ids)
    result = pred.argmax(1).item() # 获取预测概率最大的那个类别
    return result
test_data["label"] = test_data["content"].apply(infer) # 将测试集的每个文本送入模型返回结果
submit = test_data.drop(columns=["content"]) # 生成提交数据（就是把带结果的测试集丢掉内容，复制一份）
submit.to_csv("submit.csv", index=False) # 保存 CSV 文件


使用Bert模型：
import paddle
from paddlenlp.transformers import BertModel, BertTokenizer

# 载入模型和分词器
model = BertModel.from_pretrained("bert-base-chinese")
tokenizer = BertTokenizer.from_pretrained("bert-base-chinese")

# 定义 AdamW 优化器，学习率为 0.000001
optimizer = paddle.optimizer.AdamW(1e-5, parameters=model.parameters())

# 定义损失函数为交叉熵函数，计算每个 mini batch 的均值
loss_fn = paddle.nn.loss.CrossEntropyLoss(reduction='mean')

for epoch in range(30): # 训练 30 轮
    # 训练过程
    model.train() # 切换模型为训练模式
    for batch_x in tqdm(train_loader): # 每次从数据加载器读入一批(batch)数据
        X = tokenizer(batch_x["content"], max_seq_len=512, pad_to_max_seq_len=True) # 将数据转换为模块可处理的数据形式
        input_ids = paddle.to_tensor(X['input_ids'], dtype="int64") # 将 input_ids 变为张量，方便并行计算
        token_type_ids = paddle.to_tensor(X['token_type_ids'], dtype="int64") # 将 token_type_ids 变为张量
        pred = model(input_ids, token_type_ids=token_type_ids)[0] # 将数据读入模型，并得到计算后的结果
        loss = loss_fn(pred, paddle.to_tensor(batch_x["label"], dtype="int64")) # 对比预测结果与真实结果，计算损失函数的值
        loss.backward() # 反向传播，计算梯度
        optimizer.step() # 优化器根据梯度与学习率调整模型参数
        optimizer.clear_grad() # 清空梯度，避免下次梯度计算时累加

    # 验证过程
    model.eval() # 切换模型为验证模式
    val_loss = [] # 验证集数据的损失函数合集
    with paddle.no_grad(): # 在模型验证时，只做前向计算，因此不需要保存梯度信息
        for batch_x in tqdm(valid_loader): # 下面的操作与训练过程相同
            X = tokenizer(batch_x["content"], max_seq_len=512, pad_to_max_seq_len=True)
            input_ids = paddle.to_tensor(X['input_ids'], dtype="int64")
            token_type_ids = paddle.to_tensor(X['token_type_ids'], dtype="int64")
            pred = model(input_ids, token_type_ids=
在代码中，使用了PaddleNLP库中的BERT模型（`BertModel`）和BERT分词器（`BertTokenizer`）。我们还定义了AdamW优化器（`paddle.optimizer.AdamW`）
和交叉熵损失函数（`paddle.nn.loss.CrossEntropyLoss`）。
在每个训练轮次中，我们使用训练数据加载器（`train_loader`）迭代训练数据的批次。我们使用BERT分词器将输入文本转换为模型可以处理的格式，并将其转换为张量。然后，我们将输入张量传递给BERT模型，
并获得预测结果。接下来，使用交叉熵损失函数计算预测结果和真实标签之间的损失，并进行反向传播和梯度更新。
在验证过程中，我们使用验证数据加载器（`valid_loader`）迭代验证数据的批次。我们使用相同的处理步骤来获取预测结果，并计算验证集上的损失。
可以根据自己的任务和数据的特点来调整代码中的超参数，例如学习率、批次大小、训练轮次等，确保（`train_loader`和`valid_loader`）可以指向正确的数据
